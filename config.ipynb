{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 준비\n",
    "- Spark Data Information 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_n = pd.read_excel('/home/sein/sw_spark/SparkSQL_parameters_s.xlsx')\n",
    "config_f = file_n.to_csv(\"config.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameter</th>\n",
       "      <th>description</th>\n",
       "      <th>unit</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>default</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spark.broadcast.compress</td>\n",
       "      <td>Decides whether to compress broadcast variable...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spark.default.parallelism</td>\n",
       "      <td>Specifies the maximum number of partitions in ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td># of cores</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spark.driver.cores</td>\n",
       "      <td>Specifies the number of cores to use for the d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spark.executor.instances</td>\n",
       "      <td>Specifies the total number of Executor process...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spark.executor.memoryOverhead</td>\n",
       "      <td>Specifies the additional memory size to be all...</td>\n",
       "      <td>MB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49152.0</td>\n",
       "      <td>384</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spark.io.compression.zstd.bufferSize</td>\n",
       "      <td>Specifies the buffer size used in Zstd compres...</td>\n",
       "      <td>KB</td>\n",
       "      <td>16.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>32</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spark.io.compression.zstd.level</td>\n",
       "      <td>Specifies the compression level for Zstd compr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spark.locality.wait</td>\n",
       "      <td>Specifies the wait time to launch a task in a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spark.memory.fraction</td>\n",
       "      <td>Specifies the fraction of (heap space - 300MB)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spark.memory.offHeap.enabled</td>\n",
       "      <td>Decides whether to use off-heap memory for cer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spark.memory.offHeap.size</td>\n",
       "      <td>Specifies the memory size which can be used fo...</td>\n",
       "      <td>MB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49152.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spark.memory.storageFraction</td>\n",
       "      <td>Specifies the amount of storage memory immune ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spark.rdd.compress</td>\n",
       "      <td>Decides whether to compress serialized RDD par...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spark.reducer.maxSizeInFlight</td>\n",
       "      <td>Specifies the maximum size to fetch simultaneo...</td>\n",
       "      <td>MB</td>\n",
       "      <td>24.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>48</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>spark.scheduler.revive.interval</td>\n",
       "      <td>Specifies the interval for the scheduler to re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spark.shuffle.accurateBlockThreshold</td>\n",
       "      <td>Size of shuffle blocks in HighlyCompressedMapS...</td>\n",
       "      <td>MB</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>spark.shuffle.compress</td>\n",
       "      <td>Decides whether to compress map output files.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>spark.shuffle.file.buffer</td>\n",
       "      <td>Specifies in-memory buffer size for each shuff...</td>\n",
       "      <td>KB</td>\n",
       "      <td>16.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>32</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spark.shuffle.io.numConnectionsPerPeer</td>\n",
       "      <td>Specifies the amount of connections between ho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spark.shuffle.service.index.cache.entries</td>\n",
       "      <td>Max number of entries to keep in the index cac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>spark.shuffle.spill.compress</td>\n",
       "      <td>Decides whether to compress data spilled durin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>spark.speculation.interval</td>\n",
       "      <td>How often Spark will check for tasks to specul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>spark.sql.autoBroadcastJoinThreshold</td>\n",
       "      <td>Specifies the maximum size for a broadcasted t...</td>\n",
       "      <td>KB</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>8192.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>spark.sql.cartesianProductExec.buffer.in.memor...</td>\n",
       "      <td>Specifies row numbers of Cartesian cache.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>8192.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>spark.sql.codegen.aggregate.map.twolevel.enable</td>\n",
       "      <td>Decides whether to enable two-level aggregate ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>spark.sql.codegen.maxFields</td>\n",
       "      <td>Specifies the maximum field supported before a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>spark.sql.inMemoryColumnarStorage.batchSize</td>\n",
       "      <td>Specifies the size of the batch used for colum...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>spark.sql.inMemoryColumnarStorage.compressed</td>\n",
       "      <td>Decides whether to compress each column based ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>spark.sql.inMemoryColumnarStorage.partitionPru...</td>\n",
       "      <td>Decides whether to prune partition in memory.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>spark.sql.join.preferSortMergeJoin</td>\n",
       "      <td>Decides whether to use sort Merge Join instead...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>spark.sql.retainGroupColumns</td>\n",
       "      <td>Decides whether to retain group columns.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>spark.sql.shuffle.partitions</td>\n",
       "      <td>Specifies the default partition number when sh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>200</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>spark.sql.sort.enableRadixSort</td>\n",
       "      <td>Decides whether to use radix sort.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>spark.storage.memoryMapThreshold</td>\n",
       "      <td>Specifies mapped memory size when read a block...</td>\n",
       "      <td>MB</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            parameter  \\\n",
       "0                            spark.broadcast.compress   \n",
       "1                           spark.default.parallelism   \n",
       "2                                  spark.driver.cores   \n",
       "3                            spark.executor.instances   \n",
       "4                       spark.executor.memoryOverhead   \n",
       "5                spark.io.compression.zstd.bufferSize   \n",
       "6                     spark.io.compression.zstd.level   \n",
       "7                                 spark.locality.wait   \n",
       "8                               spark.memory.fraction   \n",
       "9                        spark.memory.offHeap.enabled   \n",
       "10                          spark.memory.offHeap.size   \n",
       "11                       spark.memory.storageFraction   \n",
       "12                                 spark.rdd.compress   \n",
       "13                      spark.reducer.maxSizeInFlight   \n",
       "14                    spark.scheduler.revive.interval   \n",
       "15               spark.shuffle.accurateBlockThreshold   \n",
       "16                             spark.shuffle.compress   \n",
       "17                          spark.shuffle.file.buffer   \n",
       "18             spark.shuffle.io.numConnectionsPerPeer   \n",
       "19          spark.shuffle.service.index.cache.entries   \n",
       "20                       spark.shuffle.spill.compress   \n",
       "21                         spark.speculation.interval   \n",
       "22               spark.sql.autoBroadcastJoinThreshold   \n",
       "23  spark.sql.cartesianProductExec.buffer.in.memor...   \n",
       "24    spark.sql.codegen.aggregate.map.twolevel.enable   \n",
       "25                        spark.sql.codegen.maxFields   \n",
       "26        spark.sql.inMemoryColumnarStorage.batchSize   \n",
       "27       spark.sql.inMemoryColumnarStorage.compressed   \n",
       "28  spark.sql.inMemoryColumnarStorage.partitionPru...   \n",
       "29                 spark.sql.join.preferSortMergeJoin   \n",
       "30                       spark.sql.retainGroupColumns   \n",
       "31                       spark.sql.shuffle.partitions   \n",
       "32                     spark.sql.sort.enableRadixSort   \n",
       "33                   spark.storage.memoryMapThreshold   \n",
       "\n",
       "                                          description unit     min      max  \\\n",
       "0   Decides whether to compress broadcast variable...  NaN     1.0      0.0   \n",
       "1   Specifies the maximum number of partitions in ...  NaN   100.0   1000.0   \n",
       "2   Specifies the number of cores to use for the d...  NaN     1.0     16.0   \n",
       "3   Specifies the total number of Executor process...  NaN     9.0    112.0   \n",
       "4   Specifies the additional memory size to be all...   MB     0.0  49152.0   \n",
       "5   Specifies the buffer size used in Zstd compres...   KB    16.0     96.0   \n",
       "6   Specifies the compression level for Zstd compr...  NaN     1.0      5.0   \n",
       "7   Specifies the wait time to launch a task in a ...  NaN     1.0      6.0   \n",
       "8   Specifies the fraction of (heap space - 300MB)...  NaN     0.5      0.9   \n",
       "9   Decides whether to use off-heap memory for cer...  NaN     1.0      0.0   \n",
       "10  Specifies the memory size which can be used fo...   MB     0.0  49152.0   \n",
       "11  Specifies the amount of storage memory immune ...  NaN     0.5      0.9   \n",
       "12  Decides whether to compress serialized RDD par...  NaN     1.0      0.0   \n",
       "13  Specifies the maximum size to fetch simultaneo...   MB    24.0    144.0   \n",
       "14  Specifies the interval for the scheduler to re...  NaN     1.0      5.0   \n",
       "15  Size of shuffle blocks in HighlyCompressedMapS...   MB   100.0   1000.0   \n",
       "16      Decides whether to compress map output files.  NaN     1.0      0.0   \n",
       "17  Specifies in-memory buffer size for each shuff...   KB    16.0     96.0   \n",
       "18  Specifies the amount of connections between ho...  NaN     1.0      5.0   \n",
       "19  Max number of entries to keep in the index cac...  NaN   512.0   2048.0   \n",
       "20  Decides whether to compress data spilled durin...  NaN     1.0      0.0   \n",
       "21  How often Spark will check for tasks to specul...  NaN    10.0   1000.0   \n",
       "22  Specifies the maximum size for a broadcasted t...   KB  1024.0   8192.0   \n",
       "23          Specifies row numbers of Cartesian cache.  NaN  1024.0   8192.0   \n",
       "24  Decides whether to enable two-level aggregate ...  NaN     1.0      0.0   \n",
       "25  Specifies the maximum field supported before a...  NaN    50.0    200.0   \n",
       "26  Specifies the size of the batch used for colum...  NaN  5000.0  20000.0   \n",
       "27  Decides whether to compress each column based ...  NaN     1.0      0.0   \n",
       "28      Decides whether to prune partition in memory.  NaN     1.0      0.0   \n",
       "29  Decides whether to use sort Merge Join instead...  NaN     1.0      0.0   \n",
       "30           Decides whether to retain group columns.  NaN     1.0      0.0   \n",
       "31  Specifies the default partition number when sh...  NaN   100.0   1000.0   \n",
       "32                 Decides whether to use radix sort.  NaN     1.0      0.0   \n",
       "33  Specifies mapped memory size when read a block...   MB     1.0     10.0   \n",
       "\n",
       "       default version  \n",
       "0         True   2.4.5  \n",
       "1   # of cores   2.4.5  \n",
       "2         True   2.4.5  \n",
       "3            2   2.4.5  \n",
       "4          384   2.4.5  \n",
       "5           32   2.4.5  \n",
       "6         True   2.4.5  \n",
       "7            3   2.4.5  \n",
       "8          0.6   2.4.5  \n",
       "9         True   2.4.5  \n",
       "10           0   2.4.5  \n",
       "11         0.5   2.4.5  \n",
       "12        True   2.4.5  \n",
       "13          48   2.4.5  \n",
       "14        True   2.4.5  \n",
       "15         100   2.2.2  \n",
       "16        True   2.4.5  \n",
       "17          32   2.4.5  \n",
       "18        True   2.4.5  \n",
       "19        1024   2.2.2  \n",
       "20        True   2.4.5  \n",
       "21         100   2.2.2  \n",
       "22        1024   2.4.5  \n",
       "23        4096   2.4.5  \n",
       "24        True   2.4.5  \n",
       "25         100   2.4.5  \n",
       "26       10000   2.4.5  \n",
       "27        True   2.4.5  \n",
       "28        True   2.4.5  \n",
       "29        True   2.4.5  \n",
       "30        True   2.4.5  \n",
       "31         200   2.4.5  \n",
       "32        True   2.4.5  \n",
       "33        True   2.4.5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = pd.read_csv('/home/sein/sw_spark/config.csv')\n",
    "\n",
    "\n",
    "config.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "config = config.loc[[2,3,4,9,12,13,14,19,20,21,22,23,24,25,26,27,28,29,30,31,34,35,36,37,38,39,40,41,42,43,44,\n",
    "           45,46,47]]\n",
    "config = config.reset_index(drop=True)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_min = config['min']\n",
    "config_max = config['max']\n",
    "config_name = config['parameter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32418/851966623.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  config['max'][4] = 4090\n"
     ]
    }
   ],
   "source": [
    "# spark.executor.memoryOverhead\tKnob은 max 값이 사용자의 memory 값에 따라 다름.\n",
    "\n",
    "config['max'][4] = 4090"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parameter       object\n",
       "description     object\n",
       "unit            object\n",
       "min            float64\n",
       "max            float64\n",
       "default         object\n",
       "version         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_change = [8, 11]\n",
    "\n",
    "for ind in range(len(config)):\n",
    "    if ind in ind_change:\n",
    "        config.loc[ind, 'min'] = config.loc[ind, 'min'].astype('float')\n",
    "        config.loc[ind, 'max'] = config.loc[ind, 'max'].astype('float')\n",
    "    else:\n",
    "        config.loc[ind, 'min'] = config.loc[ind, 'min'].astype('int64')\n",
    "        config.loc[ind, 'max'] = config.loc[ind, 'max'].astype('int64')\n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameter</th>\n",
       "      <th>description</th>\n",
       "      <th>unit</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>default</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spark.broadcast.compress</td>\n",
       "      <td>Decides whether to compress broadcast variable...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spark.default.parallelism</td>\n",
       "      <td>Specifies the maximum number of partitions in ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td># of cores</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spark.driver.cores</td>\n",
       "      <td>Specifies the number of cores to use for the d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spark.executor.instances</td>\n",
       "      <td>Specifies the total number of Executor process...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spark.executor.memoryOverhead</td>\n",
       "      <td>Specifies the additional memory size to be all...</td>\n",
       "      <td>MB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4090.0</td>\n",
       "      <td>384</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spark.io.compression.zstd.bufferSize</td>\n",
       "      <td>Specifies the buffer size used in Zstd compres...</td>\n",
       "      <td>KB</td>\n",
       "      <td>16.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>32</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spark.io.compression.zstd.level</td>\n",
       "      <td>Specifies the compression level for Zstd compr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spark.locality.wait</td>\n",
       "      <td>Specifies the wait time to launch a task in a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spark.memory.fraction</td>\n",
       "      <td>Specifies the fraction of (heap space - 300MB)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spark.memory.offHeap.enabled</td>\n",
       "      <td>Decides whether to use off-heap memory for cer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spark.memory.offHeap.size</td>\n",
       "      <td>Specifies the memory size which can be used fo...</td>\n",
       "      <td>MB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49152.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spark.memory.storageFraction</td>\n",
       "      <td>Specifies the amount of storage memory immune ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spark.rdd.compress</td>\n",
       "      <td>Decides whether to compress serialized RDD par...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spark.reducer.maxSizeInFlight</td>\n",
       "      <td>Specifies the maximum size to fetch simultaneo...</td>\n",
       "      <td>MB</td>\n",
       "      <td>24.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>48</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>spark.scheduler.revive.interval</td>\n",
       "      <td>Specifies the interval for the scheduler to re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spark.shuffle.accurateBlockThreshold</td>\n",
       "      <td>Size of shuffle blocks in HighlyCompressedMapS...</td>\n",
       "      <td>MB</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>spark.shuffle.compress</td>\n",
       "      <td>Decides whether to compress map output files.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>spark.shuffle.file.buffer</td>\n",
       "      <td>Specifies in-memory buffer size for each shuff...</td>\n",
       "      <td>KB</td>\n",
       "      <td>16.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>32</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spark.shuffle.io.numConnectionsPerPeer</td>\n",
       "      <td>Specifies the amount of connections between ho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spark.shuffle.service.index.cache.entries</td>\n",
       "      <td>Max number of entries to keep in the index cac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>spark.shuffle.spill.compress</td>\n",
       "      <td>Decides whether to compress data spilled durin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>spark.speculation.interval</td>\n",
       "      <td>How often Spark will check for tasks to specul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>spark.sql.autoBroadcastJoinThreshold</td>\n",
       "      <td>Specifies the maximum size for a broadcasted t...</td>\n",
       "      <td>KB</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>8192.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>spark.sql.cartesianProductExec.buffer.in.memor...</td>\n",
       "      <td>Specifies row numbers of Cartesian cache.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>8192.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>spark.sql.codegen.aggregate.map.twolevel.enable</td>\n",
       "      <td>Decides whether to enable two-level aggregate ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>spark.sql.codegen.maxFields</td>\n",
       "      <td>Specifies the maximum field supported before a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>spark.sql.inMemoryColumnarStorage.batchSize</td>\n",
       "      <td>Specifies the size of the batch used for colum...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>spark.sql.inMemoryColumnarStorage.compressed</td>\n",
       "      <td>Decides whether to compress each column based ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>spark.sql.inMemoryColumnarStorage.partitionPru...</td>\n",
       "      <td>Decides whether to prune partition in memory.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>spark.sql.join.preferSortMergeJoin</td>\n",
       "      <td>Decides whether to use sort Merge Join instead...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>spark.sql.retainGroupColumns</td>\n",
       "      <td>Decides whether to retain group columns.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>spark.sql.shuffle.partitions</td>\n",
       "      <td>Specifies the default partition number when sh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>200</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>spark.sql.sort.enableRadixSort</td>\n",
       "      <td>Decides whether to use radix sort.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>spark.storage.memoryMapThreshold</td>\n",
       "      <td>Specifies mapped memory size when read a block...</td>\n",
       "      <td>MB</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            parameter  \\\n",
       "0                            spark.broadcast.compress   \n",
       "1                           spark.default.parallelism   \n",
       "2                                  spark.driver.cores   \n",
       "3                            spark.executor.instances   \n",
       "4                       spark.executor.memoryOverhead   \n",
       "5                spark.io.compression.zstd.bufferSize   \n",
       "6                     spark.io.compression.zstd.level   \n",
       "7                                 spark.locality.wait   \n",
       "8                               spark.memory.fraction   \n",
       "9                        spark.memory.offHeap.enabled   \n",
       "10                          spark.memory.offHeap.size   \n",
       "11                       spark.memory.storageFraction   \n",
       "12                                 spark.rdd.compress   \n",
       "13                      spark.reducer.maxSizeInFlight   \n",
       "14                    spark.scheduler.revive.interval   \n",
       "15               spark.shuffle.accurateBlockThreshold   \n",
       "16                             spark.shuffle.compress   \n",
       "17                          spark.shuffle.file.buffer   \n",
       "18             spark.shuffle.io.numConnectionsPerPeer   \n",
       "19          spark.shuffle.service.index.cache.entries   \n",
       "20                       spark.shuffle.spill.compress   \n",
       "21                         spark.speculation.interval   \n",
       "22               spark.sql.autoBroadcastJoinThreshold   \n",
       "23  spark.sql.cartesianProductExec.buffer.in.memor...   \n",
       "24    spark.sql.codegen.aggregate.map.twolevel.enable   \n",
       "25                        spark.sql.codegen.maxFields   \n",
       "26        spark.sql.inMemoryColumnarStorage.batchSize   \n",
       "27       spark.sql.inMemoryColumnarStorage.compressed   \n",
       "28  spark.sql.inMemoryColumnarStorage.partitionPru...   \n",
       "29                 spark.sql.join.preferSortMergeJoin   \n",
       "30                       spark.sql.retainGroupColumns   \n",
       "31                       spark.sql.shuffle.partitions   \n",
       "32                     spark.sql.sort.enableRadixSort   \n",
       "33                   spark.storage.memoryMapThreshold   \n",
       "\n",
       "                                          description unit     min      max  \\\n",
       "0   Decides whether to compress broadcast variable...  NaN     1.0      0.0   \n",
       "1   Specifies the maximum number of partitions in ...  NaN   100.0   1000.0   \n",
       "2   Specifies the number of cores to use for the d...  NaN     1.0     16.0   \n",
       "3   Specifies the total number of Executor process...  NaN     9.0    112.0   \n",
       "4   Specifies the additional memory size to be all...   MB     0.0   4090.0   \n",
       "5   Specifies the buffer size used in Zstd compres...   KB    16.0     96.0   \n",
       "6   Specifies the compression level for Zstd compr...  NaN     1.0      5.0   \n",
       "7   Specifies the wait time to launch a task in a ...  NaN     1.0      6.0   \n",
       "8   Specifies the fraction of (heap space - 300MB)...  NaN     0.5      0.9   \n",
       "9   Decides whether to use off-heap memory for cer...  NaN     1.0      0.0   \n",
       "10  Specifies the memory size which can be used fo...   MB     0.0  49152.0   \n",
       "11  Specifies the amount of storage memory immune ...  NaN     0.5      0.9   \n",
       "12  Decides whether to compress serialized RDD par...  NaN     1.0      0.0   \n",
       "13  Specifies the maximum size to fetch simultaneo...   MB    24.0    144.0   \n",
       "14  Specifies the interval for the scheduler to re...  NaN     1.0      5.0   \n",
       "15  Size of shuffle blocks in HighlyCompressedMapS...   MB   100.0   1000.0   \n",
       "16      Decides whether to compress map output files.  NaN     1.0      0.0   \n",
       "17  Specifies in-memory buffer size for each shuff...   KB    16.0     96.0   \n",
       "18  Specifies the amount of connections between ho...  NaN     1.0      5.0   \n",
       "19  Max number of entries to keep in the index cac...  NaN   512.0   2048.0   \n",
       "20  Decides whether to compress data spilled durin...  NaN     1.0      0.0   \n",
       "21  How often Spark will check for tasks to specul...  NaN    10.0   1000.0   \n",
       "22  Specifies the maximum size for a broadcasted t...   KB  1024.0   8192.0   \n",
       "23          Specifies row numbers of Cartesian cache.  NaN  1024.0   8192.0   \n",
       "24  Decides whether to enable two-level aggregate ...  NaN     1.0      0.0   \n",
       "25  Specifies the maximum field supported before a...  NaN    50.0    200.0   \n",
       "26  Specifies the size of the batch used for colum...  NaN  5000.0  20000.0   \n",
       "27  Decides whether to compress each column based ...  NaN     1.0      0.0   \n",
       "28      Decides whether to prune partition in memory.  NaN     1.0      0.0   \n",
       "29  Decides whether to use sort Merge Join instead...  NaN     1.0      0.0   \n",
       "30           Decides whether to retain group columns.  NaN     1.0      0.0   \n",
       "31  Specifies the default partition number when sh...  NaN   100.0   1000.0   \n",
       "32                 Decides whether to use radix sort.  NaN     1.0      0.0   \n",
       "33  Specifies mapped memory size when read a block...   MB     1.0     10.0   \n",
       "\n",
       "       default version  \n",
       "0         True   2.4.5  \n",
       "1   # of cores   2.4.5  \n",
       "2         True   2.4.5  \n",
       "3            2   2.4.5  \n",
       "4          384   2.4.5  \n",
       "5           32   2.4.5  \n",
       "6         True   2.4.5  \n",
       "7            3   2.4.5  \n",
       "8          0.6   2.4.5  \n",
       "9         True   2.4.5  \n",
       "10           0   2.4.5  \n",
       "11         0.5   2.4.5  \n",
       "12        True   2.4.5  \n",
       "13          48   2.4.5  \n",
       "14        True   2.4.5  \n",
       "15         100   2.2.2  \n",
       "16        True   2.4.5  \n",
       "17          32   2.4.5  \n",
       "18        True   2.4.5  \n",
       "19        1024   2.2.2  \n",
       "20        True   2.4.5  \n",
       "21         100   2.2.2  \n",
       "22        1024   2.4.5  \n",
       "23        4096   2.4.5  \n",
       "24        True   2.4.5  \n",
       "25         100   2.4.5  \n",
       "26       10000   2.4.5  \n",
       "27        True   2.4.5  \n",
       "28        True   2.4.5  \n",
       "29        True   2.4.5  \n",
       "30        True   2.4.5  \n",
       "31         200   2.4.5  \n",
       "32        True   2.4.5  \n",
       "33        True   2.4.5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MKB_knob : 따로 단위를 정의해줘야하는 knob\n",
    "\n",
    "boolean_columns = config.loc[[0,9,12,16,20,24,27,28,29,30,32]]\n",
    "MKB_knob = config.loc[[4,5,10,13,15,17,22,33]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 9, 12, 16, 20, 24, 27, 28, 29, 30, 32], dtype='int64')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boolean_columns.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.broadcast.compress = False\n",
      "spark.default.parallelism = 1000\n",
      "spark.driver.cores = 16\n",
      "spark.executor.instances = 112\n",
      "spark.executor.memoryOverhead = 4090\n",
      "spark.io.compression.zstd.bufferSize = 96\n",
      "spark.io.compression.zstd.level = 5\n",
      "spark.locality.wait = 6\n",
      "spark.memory.fraction = 0\n",
      "spark.memory.offHeap.enabled = False\n",
      "spark.memory.offHeap.size = 49152\n",
      "spark.memory.storageFraction = 0\n",
      "spark.rdd.compress = False\n",
      "spark.reducer.maxSizeInFlight = 144\n",
      "spark.scheduler.revive.interval = 5\n",
      "spark.shuffle.accurateBlockThreshold = 1000\n",
      "spark.shuffle.compress = False\n",
      "spark.shuffle.file.buffer = 96\n",
      "spark.shuffle.io.numConnectionsPerPeer = 5\n",
      "spark.shuffle.service.index.cache.entries = 2048\n",
      "spark.shuffle.spill.compress = False\n",
      "spark.speculation.interval = 1000\n",
      "spark.sql.autoBroadcastJoinThreshold = 8192\n",
      "spark.sql.cartesianProductExec.buffer.in.memory.threshold = 8192\n",
      "spark.sql.codegen.aggregate.map.twolevel.enable = False\n",
      "spark.sql.codegen.maxFields = 200\n",
      "spark.sql.inMemoryColumnarStorage.batchSize = 20000\n",
      "spark.sql.inMemoryColumnarStorage.compressed = False\n",
      "spark.sql.inMemoryColumnarStorage.partitionPruning = False\n",
      "spark.sql.join.preferSortMergeJoin = False\n",
      "spark.sql.retainGroupColumns = False\n",
      "spark.sql.shuffle.partitions = 1000\n",
      "spark.sql.sort.enableRadixSort = False\n",
      "spark.storage.memoryMapThreshold = 10\n"
     ]
    }
   ],
   "source": [
    "### boolean_knob \n",
    "boolean_knob = [0,9,12,16,20,24,27,28,29,30,32]\n",
    "\n",
    "for i in range (len(config)):\n",
    "    value = int(config_max[i]) if i not in boolean_knob else bool(config_max[i])\n",
    "    print('{} = {}'.format(config_name[i],value))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### knob 랜덤생성 testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_min = config['min']\n",
    "config_max = config['max']\n",
    "config_name = config['parameter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/sein/sw_spark/config.ipynb Cell 14\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.172.85/home/sein/sw_spark/config.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m config[\u001b[39m'\u001b[39m\u001b[39mrandom\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m random_val  \u001b[39m# 새로운 'random' 열에 추가    \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B165.132.172.85/home/sein/sw_spark/config.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(config)):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B165.132.172.85/home/sein/sw_spark/config.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m([i]) \u001b[39mif\u001b[39;00m i \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m boolean_knob \u001b[39melse\u001b[39;00m \u001b[39mbool\u001b[39m(config_min[i])\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a number, not 'list'"
     ]
    }
   ],
   "source": [
    "### 랜덤 값으로 config 생성\n",
    "# import random\n",
    "\n",
    "# random_val = []\n",
    "# boolean_knob = [0,9,12,16,20,24,27,28,29,30,32]\n",
    "\n",
    "# for index in range(len(config)):    \n",
    "#     rand = random.uniform(config_min[index], config_max[index])\n",
    "#     # if index in boolean_knob:\n",
    "#     rand = round(rand,0)\n",
    "#     random_val.append(rand)\n",
    "# config['random'] = random_val  # 새로운 'random' 열에 추가    \n",
    "    \n",
    "# for i in range(len(config)):\n",
    "#     value = int([i]) if i not in boolean_knob else bool(config_min[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameter</th>\n",
       "      <th>description</th>\n",
       "      <th>unit</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>default</th>\n",
       "      <th>version</th>\n",
       "      <th>random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spark.broadcast.compress</td>\n",
       "      <td>Decides whether to compress broadcast variable...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spark.default.parallelism</td>\n",
       "      <td>Specifies the maximum number of partitions in ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td># of cores</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>936.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spark.driver.cores</td>\n",
       "      <td>Specifies the number of cores to use for the d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spark.executor.instances</td>\n",
       "      <td>Specifies the total number of Executor process...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spark.executor.memoryOverhead</td>\n",
       "      <td>Specifies the additional memory size to be all...</td>\n",
       "      <td>MB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49152.0</td>\n",
       "      <td>384</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>11371.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spark.io.compression.zstd.bufferSize</td>\n",
       "      <td>Specifies the buffer size used in Zstd compres...</td>\n",
       "      <td>KB</td>\n",
       "      <td>16.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>32</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>82.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spark.io.compression.zstd.level</td>\n",
       "      <td>Specifies the compression level for Zstd compr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spark.locality.wait</td>\n",
       "      <td>Specifies the wait time to launch a task in a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spark.memory.fraction</td>\n",
       "      <td>Specifies the fraction of (heap space - 300MB)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spark.memory.offHeap.enabled</td>\n",
       "      <td>Decides whether to use off-heap memory for cer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spark.memory.offHeap.size</td>\n",
       "      <td>Specifies the memory size which can be used fo...</td>\n",
       "      <td>MB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49152.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>7776.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spark.memory.storageFraction</td>\n",
       "      <td>Specifies the amount of storage memory immune ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spark.rdd.compress</td>\n",
       "      <td>Decides whether to compress serialized RDD par...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spark.reducer.maxSizeInFlight</td>\n",
       "      <td>Specifies the maximum size to fetch simultaneo...</td>\n",
       "      <td>MB</td>\n",
       "      <td>24.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>48</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>spark.scheduler.revive.interval</td>\n",
       "      <td>Specifies the interval for the scheduler to re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spark.shuffle.accurateBlockThreshold</td>\n",
       "      <td>Size of shuffle blocks in HighlyCompressedMapS...</td>\n",
       "      <td>MB</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.2.2</td>\n",
       "      <td>122.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>spark.shuffle.compress</td>\n",
       "      <td>Decides whether to compress map output files.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>spark.shuffle.file.buffer</td>\n",
       "      <td>Specifies in-memory buffer size for each shuff...</td>\n",
       "      <td>KB</td>\n",
       "      <td>16.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>32</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spark.shuffle.io.numConnectionsPerPeer</td>\n",
       "      <td>Specifies the amount of connections between ho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spark.shuffle.service.index.cache.entries</td>\n",
       "      <td>Max number of entries to keep in the index cac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.2.2</td>\n",
       "      <td>878.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>spark.shuffle.spill.compress</td>\n",
       "      <td>Decides whether to compress data spilled durin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>spark.speculation.interval</td>\n",
       "      <td>How often Spark will check for tasks to specul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.2.2</td>\n",
       "      <td>948.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>spark.sql.autoBroadcastJoinThreshold</td>\n",
       "      <td>Specifies the maximum size for a broadcasted t...</td>\n",
       "      <td>KB</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>8192.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>6828.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>spark.sql.cartesianProductExec.buffer.in.memor...</td>\n",
       "      <td>Specifies row numbers of Cartesian cache.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>8192.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>6503.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>spark.sql.codegen.aggregate.map.twolevel.enable</td>\n",
       "      <td>Decides whether to enable two-level aggregate ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>spark.sql.codegen.maxFields</td>\n",
       "      <td>Specifies the maximum field supported before a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>spark.sql.inMemoryColumnarStorage.batchSize</td>\n",
       "      <td>Specifies the size of the batch used for colum...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>9528.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>spark.sql.inMemoryColumnarStorage.compressed</td>\n",
       "      <td>Decides whether to compress each column based ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>spark.sql.inMemoryColumnarStorage.partitionPru...</td>\n",
       "      <td>Decides whether to prune partition in memory.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>spark.sql.join.preferSortMergeJoin</td>\n",
       "      <td>Decides whether to use sort Merge Join instead...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>spark.sql.retainGroupColumns</td>\n",
       "      <td>Decides whether to retain group columns.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>spark.sql.shuffle.partitions</td>\n",
       "      <td>Specifies the default partition number when sh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>200</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>424.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>spark.sql.sort.enableRadixSort</td>\n",
       "      <td>Decides whether to use radix sort.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>spark.storage.memoryMapThreshold</td>\n",
       "      <td>Specifies mapped memory size when read a block...</td>\n",
       "      <td>MB</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            parameter  \\\n",
       "0                            spark.broadcast.compress   \n",
       "1                           spark.default.parallelism   \n",
       "2                                  spark.driver.cores   \n",
       "3                            spark.executor.instances   \n",
       "4                       spark.executor.memoryOverhead   \n",
       "5                spark.io.compression.zstd.bufferSize   \n",
       "6                     spark.io.compression.zstd.level   \n",
       "7                                 spark.locality.wait   \n",
       "8                               spark.memory.fraction   \n",
       "9                        spark.memory.offHeap.enabled   \n",
       "10                          spark.memory.offHeap.size   \n",
       "11                       spark.memory.storageFraction   \n",
       "12                                 spark.rdd.compress   \n",
       "13                      spark.reducer.maxSizeInFlight   \n",
       "14                    spark.scheduler.revive.interval   \n",
       "15               spark.shuffle.accurateBlockThreshold   \n",
       "16                             spark.shuffle.compress   \n",
       "17                          spark.shuffle.file.buffer   \n",
       "18             spark.shuffle.io.numConnectionsPerPeer   \n",
       "19          spark.shuffle.service.index.cache.entries   \n",
       "20                       spark.shuffle.spill.compress   \n",
       "21                         spark.speculation.interval   \n",
       "22               spark.sql.autoBroadcastJoinThreshold   \n",
       "23  spark.sql.cartesianProductExec.buffer.in.memor...   \n",
       "24    spark.sql.codegen.aggregate.map.twolevel.enable   \n",
       "25                        spark.sql.codegen.maxFields   \n",
       "26        spark.sql.inMemoryColumnarStorage.batchSize   \n",
       "27       spark.sql.inMemoryColumnarStorage.compressed   \n",
       "28  spark.sql.inMemoryColumnarStorage.partitionPru...   \n",
       "29                 spark.sql.join.preferSortMergeJoin   \n",
       "30                       spark.sql.retainGroupColumns   \n",
       "31                       spark.sql.shuffle.partitions   \n",
       "32                     spark.sql.sort.enableRadixSort   \n",
       "33                   spark.storage.memoryMapThreshold   \n",
       "\n",
       "                                          description unit     min      max  \\\n",
       "0   Decides whether to compress broadcast variable...  NaN     1.0      0.0   \n",
       "1   Specifies the maximum number of partitions in ...  NaN   100.0   1000.0   \n",
       "2   Specifies the number of cores to use for the d...  NaN     1.0     16.0   \n",
       "3   Specifies the total number of Executor process...  NaN     9.0    112.0   \n",
       "4   Specifies the additional memory size to be all...   MB     0.0  49152.0   \n",
       "5   Specifies the buffer size used in Zstd compres...   KB    16.0     96.0   \n",
       "6   Specifies the compression level for Zstd compr...  NaN     1.0      5.0   \n",
       "7   Specifies the wait time to launch a task in a ...  NaN     1.0      6.0   \n",
       "8   Specifies the fraction of (heap space - 300MB)...  NaN     0.5      0.9   \n",
       "9   Decides whether to use off-heap memory for cer...  NaN     1.0      0.0   \n",
       "10  Specifies the memory size which can be used fo...   MB     0.0  49152.0   \n",
       "11  Specifies the amount of storage memory immune ...  NaN     0.5      0.9   \n",
       "12  Decides whether to compress serialized RDD par...  NaN     1.0      0.0   \n",
       "13  Specifies the maximum size to fetch simultaneo...   MB    24.0    144.0   \n",
       "14  Specifies the interval for the scheduler to re...  NaN     1.0      5.0   \n",
       "15  Size of shuffle blocks in HighlyCompressedMapS...   MB   100.0   1000.0   \n",
       "16      Decides whether to compress map output files.  NaN     1.0      0.0   \n",
       "17  Specifies in-memory buffer size for each shuff...   KB    16.0     96.0   \n",
       "18  Specifies the amount of connections between ho...  NaN     1.0      5.0   \n",
       "19  Max number of entries to keep in the index cac...  NaN   512.0   2048.0   \n",
       "20  Decides whether to compress data spilled durin...  NaN     1.0      0.0   \n",
       "21  How often Spark will check for tasks to specul...  NaN    10.0   1000.0   \n",
       "22  Specifies the maximum size for a broadcasted t...   KB  1024.0   8192.0   \n",
       "23          Specifies row numbers of Cartesian cache.  NaN  1024.0   8192.0   \n",
       "24  Decides whether to enable two-level aggregate ...  NaN     1.0      0.0   \n",
       "25  Specifies the maximum field supported before a...  NaN    50.0    200.0   \n",
       "26  Specifies the size of the batch used for colum...  NaN  5000.0  20000.0   \n",
       "27  Decides whether to compress each column based ...  NaN     1.0      0.0   \n",
       "28      Decides whether to prune partition in memory.  NaN     1.0      0.0   \n",
       "29  Decides whether to use sort Merge Join instead...  NaN     1.0      0.0   \n",
       "30           Decides whether to retain group columns.  NaN     1.0      0.0   \n",
       "31  Specifies the default partition number when sh...  NaN   100.0   1000.0   \n",
       "32                 Decides whether to use radix sort.  NaN     1.0      0.0   \n",
       "33  Specifies mapped memory size when read a block...   MB     1.0     10.0   \n",
       "\n",
       "       default version   random  \n",
       "0         True   2.4.5      0.0  \n",
       "1   # of cores   2.4.5    936.0  \n",
       "2         True   2.4.5      5.0  \n",
       "3            2   2.4.5     59.0  \n",
       "4          384   2.4.5  11371.0  \n",
       "5           32   2.4.5     82.0  \n",
       "6         True   2.4.5      3.0  \n",
       "7            3   2.4.5      1.0  \n",
       "8          0.6   2.4.5      1.0  \n",
       "9         True   2.4.5      0.0  \n",
       "10           0   2.4.5   7776.0  \n",
       "11         0.5   2.4.5      1.0  \n",
       "12        True   2.4.5      0.0  \n",
       "13          48   2.4.5     30.0  \n",
       "14        True   2.4.5      2.0  \n",
       "15         100   2.2.2    122.0  \n",
       "16        True   2.4.5      0.0  \n",
       "17          32   2.4.5     43.0  \n",
       "18        True   2.4.5      4.0  \n",
       "19        1024   2.2.2    878.0  \n",
       "20        True   2.4.5      1.0  \n",
       "21         100   2.2.2    948.0  \n",
       "22        1024   2.4.5   6828.0  \n",
       "23        4096   2.4.5   6503.0  \n",
       "24        True   2.4.5      1.0  \n",
       "25         100   2.4.5     81.0  \n",
       "26       10000   2.4.5   9528.0  \n",
       "27        True   2.4.5      0.0  \n",
       "28        True   2.4.5      1.0  \n",
       "29        True   2.4.5      1.0  \n",
       "30        True   2.4.5      0.0  \n",
       "31         200   2.4.5    424.0  \n",
       "32        True   2.4.5      1.0  \n",
       "33        True   2.4.5     10.0  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 랜덤 값으로 config 생성\n",
    "\n",
    "# import random\n",
    "\n",
    "# random_val = []\n",
    "# boolean_knob = [0,9,12,16, 20,24,27,28,29,30,32]\n",
    "# spec_knob = [4, 5, 10, 13, 15, 17, 22, 33]\n",
    "\n",
    "# # config_min , config_max\n",
    "\n",
    "\n",
    "# for index in range(len(config)):    \n",
    "#     rand = random.uniform(config_min[index], config_max[index])\n",
    "#     # if index in boolean_knob:\n",
    "#     rand = round(rand,0)\n",
    "#     random_val.append(rand)  \n",
    "#     int(random_val[index]) if index not in boolean_knob else bool(random_val[index])   \n",
    "# config['random'] = random_val  \n",
    "\n",
    "# for i in range(len(config)):\n",
    "#     value = int(config['random'][i]) if i not in boolean_knob else bool(config['random'][i])\n",
    "#     # if config['unit'][i] == 'MB':\n",
    "#     #   value = str(config['random']) + 'm'\n",
    "#     # elif config['unit'][i] == 'KB':\n",
    "#     #   value = str(config['random']) + 'k'\n",
    "      \n",
    "    \n",
    "#       # 새로운 'random' 열에 추가\n",
    "#     print('{} = {}'.format(config_name[i],value))\n",
    "# # config['random'] = random_val  # 새로운 'random' 열에 추가\n",
    "\n",
    "\n",
    "# # 'unit' 열에서 'MB'인 경우 'm' 추가, 'KB'인 경우 'k' 추가\n",
    "# # for index, row in df.iterrows():\n",
    "# #     if index in [4, 5, 10, 13, 15, 17, 22, 33]:  # 원하는 인덱스 리스트\n",
    "# #         if row['unit'] == 'MB':\n",
    "# #             df.at[index, 'value'] = str(row['value']) + 'm'\n",
    "# #         elif row['unit'] == 'KB':\n",
    "# #             df.at[index, 'value'] = str(row['value']) + 'k'\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knob Value randeom generation\n",
    "- boolean type : true or false\n",
    "- spec_knob : 단위가 필요한 knob (k, m ...)\n",
    "- 테스트 시 문제 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark.broadcast.compress = True\n",
      "spark.default.parallelism = 166\n",
      "spark.driver.cores = 7\n",
      "spark.executor.instances = 73\n",
      "spark.executor.memoryOverhead = 3205m\n",
      "spark.io.compression.zstd.bufferSize = 65k\n",
      "spark.io.compression.zstd.level = 2\n",
      "spark.locality.wait = 6\n",
      "spark.memory.fraction = 1\n",
      "spark.memory.offHeap.enabled = True\n",
      "spark.memory.offHeap.size = 21546m\n",
      "spark.memory.storageFraction = 1\n",
      "spark.rdd.compress = True\n",
      "spark.reducer.maxSizeInFlight = 92m\n",
      "spark.scheduler.revive.interval = 2\n",
      "spark.shuffle.accurateBlockThreshold = 508m\n",
      "spark.shuffle.compress = False\n",
      "spark.shuffle.file.buffer = 94k\n",
      "spark.shuffle.io.numConnectionsPerPeer = 3\n",
      "spark.shuffle.service.index.cache.entries = 1450\n",
      "spark.shuffle.spill.compress = False\n",
      "spark.speculation.interval = 300\n",
      "spark.sql.autoBroadcastJoinThreshold = 7328k\n",
      "spark.sql.cartesianProductExec.buffer.in.memory.threshold = 6645\n",
      "spark.sql.codegen.aggregate.map.twolevel.enable = True\n",
      "spark.sql.codegen.maxFields = 83\n",
      "spark.sql.inMemoryColumnarStorage.batchSize = 16630\n",
      "spark.sql.inMemoryColumnarStorage.compressed = False\n",
      "spark.sql.inMemoryColumnarStorage.partitionPruning = False\n",
      "spark.sql.join.preferSortMergeJoin = True\n",
      "spark.sql.retainGroupColumns = True\n",
      "spark.sql.shuffle.partitions = 696\n",
      "spark.sql.sort.enableRadixSort = True\n",
      "spark.storage.memoryMapThreshold = 3m\n"
     ]
    }
   ],
   "source": [
    "### 랜덤 값으로 config 생성\n",
    "\n",
    "import random\n",
    "\n",
    "random_val = []\n",
    "boolean_knob = [0,9,12,16, 20,24,27,28,29,30,32]\n",
    "spec_knob = [4, 5, 10, 13, 15, 17, 22, 33]\n",
    "\n",
    "# config_min , config_max\n",
    "\n",
    "\n",
    "for index in range(len(config)):    \n",
    "    rand = random.uniform(config_min[index], config_max[index])\n",
    "    # if index in boolean_knob:\n",
    "    rand = round(rand,0)\n",
    "    random_val.append(rand)  \n",
    "    int(random_val[index]) if index not in boolean_knob else bool(random_val[index])   \n",
    "config['random'] = random_val  \n",
    "\n",
    "for i in range(len(config)):\n",
    "    value = int(config['random'][i]) if i not in boolean_knob else bool(config['random'][i])\n",
    "    if config['unit'][i] == 'MB':\n",
    "      value = str(round(config['random'][i])) + 'm'\n",
    "    elif config['unit'][i] == 'KB':\n",
    "      value = str(round(config['random'][i])) + 'k'\n",
    "      \n",
    "    \n",
    "      # 새로운 'random' 열에 추가\n",
    "    print('{} = {}'.format(config_name[i],value))\n",
    "# config['random'] = random_val  # 새로운 'random' 열에 추가\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameter</th>\n",
       "      <th>description</th>\n",
       "      <th>unit</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>default</th>\n",
       "      <th>version</th>\n",
       "      <th>random</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spark.broadcast.compress</td>\n",
       "      <td>Decides whether to compress broadcast variable...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spark.default.parallelism</td>\n",
       "      <td>Specifies the maximum number of partitions in ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td># of cores</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>613.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spark.driver.cores</td>\n",
       "      <td>Specifies the number of cores to use for the d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spark.executor.instances</td>\n",
       "      <td>Specifies the total number of Executor process...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>81.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spark.executor.memoryOverhead</td>\n",
       "      <td>Specifies the additional memory size to be all...</td>\n",
       "      <td>MB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4090.0</td>\n",
       "      <td>384</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>3700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spark.io.compression.zstd.bufferSize</td>\n",
       "      <td>Specifies the buffer size used in Zstd compres...</td>\n",
       "      <td>KB</td>\n",
       "      <td>16.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>32</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>spark.io.compression.zstd.level</td>\n",
       "      <td>Specifies the compression level for Zstd compr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>spark.locality.wait</td>\n",
       "      <td>Specifies the wait time to launch a task in a ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spark.memory.fraction</td>\n",
       "      <td>Specifies the fraction of (heap space - 300MB)...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spark.memory.offHeap.enabled</td>\n",
       "      <td>Decides whether to use off-heap memory for cer...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>spark.memory.offHeap.size</td>\n",
       "      <td>Specifies the memory size which can be used fo...</td>\n",
       "      <td>MB</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49152.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>45307.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>spark.memory.storageFraction</td>\n",
       "      <td>Specifies the amount of storage memory immune ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>spark.rdd.compress</td>\n",
       "      <td>Decides whether to compress serialized RDD par...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>spark.reducer.maxSizeInFlight</td>\n",
       "      <td>Specifies the maximum size to fetch simultaneo...</td>\n",
       "      <td>MB</td>\n",
       "      <td>24.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>48</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>spark.scheduler.revive.interval</td>\n",
       "      <td>Specifies the interval for the scheduler to re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>spark.shuffle.accurateBlockThreshold</td>\n",
       "      <td>Size of shuffle blocks in HighlyCompressedMapS...</td>\n",
       "      <td>MB</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.2.2</td>\n",
       "      <td>378.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>spark.shuffle.compress</td>\n",
       "      <td>Decides whether to compress map output files.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>spark.shuffle.file.buffer</td>\n",
       "      <td>Specifies in-memory buffer size for each shuff...</td>\n",
       "      <td>KB</td>\n",
       "      <td>16.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>32</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>spark.shuffle.io.numConnectionsPerPeer</td>\n",
       "      <td>Specifies the amount of connections between ho...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>spark.shuffle.service.index.cache.entries</td>\n",
       "      <td>Max number of entries to keep in the index cac...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>512.0</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.2.2</td>\n",
       "      <td>1248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>spark.shuffle.spill.compress</td>\n",
       "      <td>Decides whether to compress data spilled durin...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>spark.speculation.interval</td>\n",
       "      <td>How often Spark will check for tasks to specul...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.2.2</td>\n",
       "      <td>704.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>spark.sql.autoBroadcastJoinThreshold</td>\n",
       "      <td>Specifies the maximum size for a broadcasted t...</td>\n",
       "      <td>KB</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>8192.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>6930.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>spark.sql.cartesianProductExec.buffer.in.memor...</td>\n",
       "      <td>Specifies row numbers of Cartesian cache.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>8192.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>2088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>spark.sql.codegen.aggregate.map.twolevel.enable</td>\n",
       "      <td>Decides whether to enable two-level aggregate ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>spark.sql.codegen.maxFields</td>\n",
       "      <td>Specifies the maximum field supported before a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>100</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>spark.sql.inMemoryColumnarStorage.batchSize</td>\n",
       "      <td>Specifies the size of the batch used for colum...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>10000</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>18505.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>spark.sql.inMemoryColumnarStorage.compressed</td>\n",
       "      <td>Decides whether to compress each column based ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>spark.sql.inMemoryColumnarStorage.partitionPru...</td>\n",
       "      <td>Decides whether to prune partition in memory.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>spark.sql.join.preferSortMergeJoin</td>\n",
       "      <td>Decides whether to use sort Merge Join instead...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>spark.sql.retainGroupColumns</td>\n",
       "      <td>Decides whether to retain group columns.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>spark.sql.shuffle.partitions</td>\n",
       "      <td>Specifies the default partition number when sh...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>200</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>926.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>spark.sql.sort.enableRadixSort</td>\n",
       "      <td>Decides whether to use radix sort.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>spark.storage.memoryMapThreshold</td>\n",
       "      <td>Specifies mapped memory size when read a block...</td>\n",
       "      <td>MB</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>True</td>\n",
       "      <td>2.4.5</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            parameter  \\\n",
       "0                            spark.broadcast.compress   \n",
       "1                           spark.default.parallelism   \n",
       "2                                  spark.driver.cores   \n",
       "3                            spark.executor.instances   \n",
       "4                       spark.executor.memoryOverhead   \n",
       "5                spark.io.compression.zstd.bufferSize   \n",
       "6                     spark.io.compression.zstd.level   \n",
       "7                                 spark.locality.wait   \n",
       "8                               spark.memory.fraction   \n",
       "9                        spark.memory.offHeap.enabled   \n",
       "10                          spark.memory.offHeap.size   \n",
       "11                       spark.memory.storageFraction   \n",
       "12                                 spark.rdd.compress   \n",
       "13                      spark.reducer.maxSizeInFlight   \n",
       "14                    spark.scheduler.revive.interval   \n",
       "15               spark.shuffle.accurateBlockThreshold   \n",
       "16                             spark.shuffle.compress   \n",
       "17                          spark.shuffle.file.buffer   \n",
       "18             spark.shuffle.io.numConnectionsPerPeer   \n",
       "19          spark.shuffle.service.index.cache.entries   \n",
       "20                       spark.shuffle.spill.compress   \n",
       "21                         spark.speculation.interval   \n",
       "22               spark.sql.autoBroadcastJoinThreshold   \n",
       "23  spark.sql.cartesianProductExec.buffer.in.memor...   \n",
       "24    spark.sql.codegen.aggregate.map.twolevel.enable   \n",
       "25                        spark.sql.codegen.maxFields   \n",
       "26        spark.sql.inMemoryColumnarStorage.batchSize   \n",
       "27       spark.sql.inMemoryColumnarStorage.compressed   \n",
       "28  spark.sql.inMemoryColumnarStorage.partitionPru...   \n",
       "29                 spark.sql.join.preferSortMergeJoin   \n",
       "30                       spark.sql.retainGroupColumns   \n",
       "31                       spark.sql.shuffle.partitions   \n",
       "32                     spark.sql.sort.enableRadixSort   \n",
       "33                   spark.storage.memoryMapThreshold   \n",
       "\n",
       "                                          description unit     min      max  \\\n",
       "0   Decides whether to compress broadcast variable...  NaN     1.0      0.0   \n",
       "1   Specifies the maximum number of partitions in ...  NaN   100.0   1000.0   \n",
       "2   Specifies the number of cores to use for the d...  NaN     1.0     16.0   \n",
       "3   Specifies the total number of Executor process...  NaN     9.0    112.0   \n",
       "4   Specifies the additional memory size to be all...   MB     0.0   4090.0   \n",
       "5   Specifies the buffer size used in Zstd compres...   KB    16.0     96.0   \n",
       "6   Specifies the compression level for Zstd compr...  NaN     1.0      5.0   \n",
       "7   Specifies the wait time to launch a task in a ...  NaN     1.0      6.0   \n",
       "8   Specifies the fraction of (heap space - 300MB)...  NaN     0.5      0.9   \n",
       "9   Decides whether to use off-heap memory for cer...  NaN     1.0      0.0   \n",
       "10  Specifies the memory size which can be used fo...   MB     0.0  49152.0   \n",
       "11  Specifies the amount of storage memory immune ...  NaN     0.5      0.9   \n",
       "12  Decides whether to compress serialized RDD par...  NaN     1.0      0.0   \n",
       "13  Specifies the maximum size to fetch simultaneo...   MB    24.0    144.0   \n",
       "14  Specifies the interval for the scheduler to re...  NaN     1.0      5.0   \n",
       "15  Size of shuffle blocks in HighlyCompressedMapS...   MB   100.0   1000.0   \n",
       "16      Decides whether to compress map output files.  NaN     1.0      0.0   \n",
       "17  Specifies in-memory buffer size for each shuff...   KB    16.0     96.0   \n",
       "18  Specifies the amount of connections between ho...  NaN     1.0      5.0   \n",
       "19  Max number of entries to keep in the index cac...  NaN   512.0   2048.0   \n",
       "20  Decides whether to compress data spilled durin...  NaN     1.0      0.0   \n",
       "21  How often Spark will check for tasks to specul...  NaN    10.0   1000.0   \n",
       "22  Specifies the maximum size for a broadcasted t...   KB  1024.0   8192.0   \n",
       "23          Specifies row numbers of Cartesian cache.  NaN  1024.0   8192.0   \n",
       "24  Decides whether to enable two-level aggregate ...  NaN     1.0      0.0   \n",
       "25  Specifies the maximum field supported before a...  NaN    50.0    200.0   \n",
       "26  Specifies the size of the batch used for colum...  NaN  5000.0  20000.0   \n",
       "27  Decides whether to compress each column based ...  NaN     1.0      0.0   \n",
       "28      Decides whether to prune partition in memory.  NaN     1.0      0.0   \n",
       "29  Decides whether to use sort Merge Join instead...  NaN     1.0      0.0   \n",
       "30           Decides whether to retain group columns.  NaN     1.0      0.0   \n",
       "31  Specifies the default partition number when sh...  NaN   100.0   1000.0   \n",
       "32                 Decides whether to use radix sort.  NaN     1.0      0.0   \n",
       "33  Specifies mapped memory size when read a block...   MB     1.0     10.0   \n",
       "\n",
       "       default version   random  \n",
       "0         True   2.4.5      0.0  \n",
       "1   # of cores   2.4.5    613.0  \n",
       "2         True   2.4.5     13.0  \n",
       "3            2   2.4.5     81.0  \n",
       "4          384   2.4.5   3700.0  \n",
       "5           32   2.4.5     61.0  \n",
       "6         True   2.4.5      3.0  \n",
       "7            3   2.4.5      4.0  \n",
       "8          0.6   2.4.5      1.0  \n",
       "9         True   2.4.5      0.0  \n",
       "10           0   2.4.5  45307.0  \n",
       "11         0.5   2.4.5      1.0  \n",
       "12        True   2.4.5      1.0  \n",
       "13          48   2.4.5     85.0  \n",
       "14        True   2.4.5      3.0  \n",
       "15         100   2.2.2    378.0  \n",
       "16        True   2.4.5      1.0  \n",
       "17          32   2.4.5     53.0  \n",
       "18        True   2.4.5      2.0  \n",
       "19        1024   2.2.2   1248.0  \n",
       "20        True   2.4.5      0.0  \n",
       "21         100   2.2.2    704.0  \n",
       "22        1024   2.4.5   6930.0  \n",
       "23        4096   2.4.5   2088.0  \n",
       "24        True   2.4.5      0.0  \n",
       "25         100   2.4.5     57.0  \n",
       "26       10000   2.4.5  18505.0  \n",
       "27        True   2.4.5      0.0  \n",
       "28        True   2.4.5      1.0  \n",
       "29        True   2.4.5      1.0  \n",
       "30        True   2.4.5      0.0  \n",
       "31         200   2.4.5    926.0  \n",
       "32        True   2.4.5      0.0  \n",
       "33        True   2.4.5      6.0  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
